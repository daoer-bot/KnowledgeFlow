"""
å†…å®¹å¤„ç†å·¥å…·
åŒ…å« RSS è§£æã€ç½‘é¡µæŠ“å–ã€å…¨æ–‡æå–ç­‰åŠŸèƒ½
"""

import feedparser
import trafilatura
import yaml
import logging
import requests
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime

logger = logging.getLogger(__name__)


class RSSFeedReader:
    """RSS è®¢é˜…æºè¯»å–å™¨"""
    
    def __init__(self, config_path: str = "config/rss_feeds.yaml"):
        """
        åˆå§‹åŒ– RSS è¯»å–å™¨
        
        Args:
            config_path: RSS é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.feeds = []
        self.config = {}
        self._load_config()
    
    def _load_config(self):
        """åŠ è½½ RSS é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                self.feeds = [feed for feed in data.get('feeds', []) if feed.get('enabled', True)]
                self.config = data.get('collection', {})
                logger.info(f"Loaded {len(self.feeds)} RSS feeds")
        except Exception as e:
            logger.error(f"Failed to load RSS config: {str(e)}")
            self.feeds = []
    
    def fetch_feed(self, feed_url: str, max_items: int = 10) -> List[Dict[str, Any]]:
        """
        æŠ“å–å•ä¸ª RSS æº
        
        Args:
            feed_url: RSS æº URL
            max_items: æœ€å¤§æ¡ç›®æ•°
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            feed = feedparser.parse(feed_url)
            
            if feed.bozo:  # è§£æå‡ºé”™
                logger.warning(f"Feed parsing error for {feed_url}: {feed.bozo_exception}")
            
            items = []
            for entry in feed.entries[:max_items]:
                item = {
                    'title': entry.get('title', 'Untitled'),
                    'url': entry.get('link', ''),
                    'summary': entry.get('summary', ''),
                    'published': entry.get('published', ''),
                    'author': entry.get('author', ''),
                }
                
                # å°è¯•æå–å…¨æ–‡
                if item['url']:
                    full_content = self.extract_content(item['url'])
                    if full_content:
                        item['content'] = full_content
                
                items.append(item)
            
            logger.info(f"Fetched {len(items)} items from {feed_url}")
            return items
            
        except Exception as e:
            logger.error(f"Error fetching feed {feed_url}: {str(e)}")
            return []
    
    def fetch_hackernews_api(self, api_type: str = "top", count: int = 5) -> List[Dict[str, Any]]:
        """
        é€šè¿‡ Hacker News API æŠ“å–å†…å®¹
        
        Args:
            api_type: API ç±»å‹ (top/new/best)
            count: æŠ“å–æ•°é‡
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        try:
            # è·å–æ•…äº‹ ID åˆ—è¡¨
            api_urls = {
                'top': 'https://hacker-news.firebaseio.com/v0/topstories.json',
                'new': 'https://hacker-news.firebaseio.com/v0/newstories.json',
                'best': 'https://hacker-news.firebaseio.com/v0/beststories.json'
            }
            
            api_url = api_urls.get(api_type, api_urls['top'])
            response = requests.get(api_url, timeout=10)
            response.raise_for_status()
            story_ids = response.json()[:count]
            
            items = []
            for story_id in story_ids:
                # è·å–æ•…äº‹è¯¦æƒ…
                story_response = requests.get(
                    f'https://hacker-news.firebaseio.com/v0/item/{story_id}.json',
                    timeout=10
                )
                
                if story_response.ok:
                    story = story_response.json()
                    if story and story.get('title'):
                        url = story.get('url', f'https://news.ycombinator.com/item?id={story_id}')
                        
                        # æ„å»ºå†…å®¹é¡¹
                        item = {
                            'title': story.get('title', 'Untitled'),
                            'url': url,
                            'summary': f"â¬†ï¸ {story.get('score', 0)} points | ğŸ’¬ {story.get('descendants', 0)} comments",
                            'author': story.get('by', 'unknown'),
                            'published': datetime.fromtimestamp(story.get('time', 0)).isoformat() if story.get('time') else '',
                        }
                        
                        # å¦‚æœæœ‰å¤–éƒ¨ URLï¼Œå°è¯•æå–å…¨æ–‡
                        if url and not url.startswith('https://news.ycombinator.com'):
                            full_content = self.extract_content(url)
                            if full_content:
                                item['content'] = full_content
                        
                        items.append(item)
            
            logger.info(f"Fetched {len(items)} items from Hacker News API ({api_type})")
            return items
            
        except Exception as e:
            logger.error(f"Error fetching Hacker News API: {str(e)}")
            return []
    
    def fetch_all_feeds(self) -> List[Dict[str, Any]]:
        """
        æŠ“å–æ‰€æœ‰é…ç½®çš„ RSS æº
        
        Returns:
            åŒ…å«æ¥æºä¿¡æ¯çš„æ–‡ç« åˆ—è¡¨
        """
        all_items = []
        max_items = self.config.get('max_items_per_feed', 10)
        
        for feed_config in self.feeds:
            feed_name = feed_config['name']
            category = feed_config.get('category', 'general')
            feed_type = feed_config.get('type', 'rss')
            
            logger.info(f"Fetching feed: {feed_name} (type: {feed_type})")
            
            # æ ¹æ®ç±»å‹é€‰æ‹©æŠ“å–æ–¹æ³•
            if feed_type == 'hackernews_api':
                # Hacker News API æº
                api_type = feed_config.get('api_type', 'top')
                fetch_count = feed_config.get('fetch_count', 5)
                items = self.fetch_hackernews_api(api_type, fetch_count)
            else:
                # æ ‡å‡† RSS æº
                feed_url = feed_config['url']
                items = self.fetch_feed(feed_url, max_items)
            
            # æ·»åŠ æ¥æºä¿¡æ¯
            for item in items:
                item['source'] = feed_name
                item['source_type'] = feed_type
                item['category_hint'] = category
            
            all_items.extend(items)
        
        logger.info(f"Total fetched: {len(all_items)} items from {len(self.feeds)} feeds")
        return all_items
    
    @staticmethod
    def extract_content(url: str, timeout: int = 30) -> Optional[str]:
        """
        ä» URL æå–å…¨æ–‡å†…å®¹
        
        Args:
            url: æ–‡ç«  URL
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            downloaded = trafilatura.fetch_url(url)
            if downloaded:
                content = trafilatura.extract(
                    downloaded,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False
                )
                return content
        except Exception as e:
            logger.warning(f"Failed to extract content from {url}: {str(e)}")
        
        return None


class WebScraper:
    """ç½‘é¡µå†…å®¹æŠ“å–å™¨"""
    
    @staticmethod
    def scrape_url(url: str) -> Optional[Dict[str, Any]]:
        """
        æŠ“å–æŒ‡å®š URL çš„å†…å®¹
        
        Args:
            url: ç›®æ ‡ URL
            
        Returns:
            åŒ…å«æ ‡é¢˜å’Œå†…å®¹çš„å­—å…¸
        """
        try:
            # ä¸‹è½½ç½‘é¡µ
            downloaded = trafilatura.fetch_url(url)
            if not downloaded:
                logger.error(f"Failed to download {url}")
                return None
            
            # æå–å†…å®¹
            content = trafilatura.extract(
                downloaded,
                include_comments=False,
                include_tables=True,
                no_fallback=False,
                output_format='json'
            )
            
            if not content:
                logger.error(f"Failed to extract content from {url}")
                return None
            
            # trafilatura è¿”å› JSON å­—ç¬¦ä¸²
            import json
            data = json.loads(content)
            
            result = {
                'title': data.get('title', 'Untitled'),
                'url': url,
                'content': data.get('text', ''),
                'author': data.get('author', ''),
                'date': data.get('date', ''),
                'source': data.get('sitename', 'Unknown'),
                'source_type': 'web'
            }
            
            logger.info(f"Scraped {len(result['content'])} characters from {url}")
            return result
            
        except Exception as e:
            logger.error(f"Error scraping {url}: {str(e)}")
            return None
    
    @staticmethod
    def validate_url(url: str) -> bool:
        """
        éªŒè¯ URL æ ¼å¼
        
        Args:
            url: URL å­—ç¬¦ä¸²
            
        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        import re
        pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain
            r'localhost|'  # localhost
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # IP
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE
        )
        return pattern.match(url) is not None


class ContentProcessor:
    """å†…å®¹å¤„ç†è¾…åŠ©ç±»"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬
        
        - ç§»é™¤å¤šä½™ç©ºç™½
        - ç»Ÿä¸€æ¢è¡Œç¬¦
        """
        if not text:
            return ""
        
        import re
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç»Ÿä¸€æ¢è¡Œ
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        return text.strip()
    
    @staticmethod
    def extract_keywords(text: str, top_n: int = 5) -> List[str]:
        """
        æå–å…³é”®è¯ï¼ˆç®€å•ç‰ˆæœ¬ï¼ŒåŸºäºè¯é¢‘ï¼‰
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            top_n: è¿”å›å‰Nä¸ªå…³é”®è¯
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        import re
        from collections import Counter
        
        # ç®€å•åˆ†è¯ï¼ˆä»…ä½œæ¼”ç¤ºï¼Œå®é™…åº”ä½¿ç”¨ jieba ç­‰å·¥å…·ï¼‰
        words = re.findall(r'\w+', text.lower())
        
        # è¿‡æ»¤åœç”¨è¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        words = [w for w in words if w not in stopwords and len(w) > 2]
        
        # ç»Ÿè®¡è¯é¢‘
        counter = Counter(words)
        
        return [word for word, count in counter.most_common(top_n)]
    
    @staticmethod
    def count_words(text: str) -> int:
        """
        ç»Ÿè®¡å­—æ•°
        
        ä¸­æ–‡æŒ‰å­—ç¬¦æ•°ï¼Œè‹±æ–‡æŒ‰å•è¯æ•°
        """
        if not text:
            return 0
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
        
        # ç»Ÿè®¡è‹±æ–‡å•è¯
        import re
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        
        return chinese_chars + english_words
    
    @staticmethod
    def truncate_text(text: str, max_length: int = 500, suffix: str = "...") -> str:
        """
        æˆªæ–­æ–‡æœ¬
        
        Args:
            text: åŸæ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            suffix: åç¼€
            
        Returns:
            æˆªæ–­åçš„æ–‡æœ¬
        """
        if len(text) <= max_length:
            return text
        
        return text[:max_length - len(suffix)] + suffix
    
    @staticmethod
    def format_content_card(content_data: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–å†…å®¹å¡ç‰‡ï¼ˆç”¨äºé¢‘é“æ¶ˆæ¯ï¼‰
        
        Args:
            content_data: å†…å®¹æ•°æ®
            
        Returns:
            æ ¼å¼åŒ–çš„ Markdown æ–‡æœ¬
        """
        title = content_data.get('title', 'Untitled')
        url = content_data.get('url', '')
        source = content_data.get('source', 'Unknown')
        summary = content_data.get('summary_paragraph', content_data.get('summary', ''))
        category = content_data.get('category', '')
        tags = content_data.get('tags', {})
        
        # æ„å»ºå¡ç‰‡
        card = f"ğŸ“Œ **{title}**\n\n"
        
        if summary:
            card += f"ğŸ“ {summary}\n\n"
        
        # æ ‡ç­¾
        if tags:
            tag_list = []
            for tag_type, tag_values in tags.items():
                if isinstance(tag_values, list):
                    tag_list.extend(tag_values)
            
            if tag_list:
                card += f"ğŸ·ï¸ {' '.join(['#' + t for t in tag_list[:5]])}\n\n"
        
        # æ¥æºå’Œåˆ†ç±»
        card += f"ğŸ“š {source}"
        if category:
            card += f" | {category}"
        card += "\n"
        
        if url:
            card += f"ğŸ”— {url}\n"
        
        return card


# ä¾¿æ·å‡½æ•°
def get_rss_reader() -> RSSFeedReader:
    """è·å– RSS è¯»å–å™¨å®ä¾‹"""
    return RSSFeedReader()


def get_web_scraper() -> WebScraper:
    """è·å–ç½‘é¡µæŠ“å–å™¨å®ä¾‹"""
    return WebScraper()