#!/usr/bin/env python3
"""
Web Scraper Agent - å“åº”ç”¨æˆ·è¯·æ±‚æŠ“å–æŒ‡å®šç½‘é¡µ

åŠŸèƒ½ï¼š
- ç›‘å¬ scraper-requests é¢‘é“
- è§£æç”¨æˆ·å‘é€çš„ URL
- æŠ“å–ç½‘é¡µå†…å®¹
- å‘é€ content.discovered äº‹ä»¶
- å›å¤ç”¨æˆ·æŠ“å–ç»“æœ
"""

import asyncio
import sys
import re
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "src"))
sys.path.insert(0, str(Path(__file__).parent.parent))

from openagents.agents.worker_agent import WorkerAgent, on_event
from tools.content_tools import WebScraper
from tools.database import get_database
import logging

logger = logging.getLogger(__name__)


class WebScraperAgent(WorkerAgent):
    """Web Scraper Agent - æŒ‰éœ€æŠ“å–ç½‘é¡µ"""
    
    default_agent_id = "ç½‘é¡µæŠ“å–ä»£ç†"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.scraper = WebScraper()
        self.db = get_database()
    
    async def on_startup(self):
        """Agent å¯åŠ¨æ—¶æ‰§è¡Œ"""
        logger.info("Web Scraper Agent started")
        
        await self._send_channel_message(
            "é€šç”¨é¢‘é“",
            "ğŸ¤– Web Scraper å·²ä¸Šçº¿\n\nä½¿ç”¨æ–¹æ³•ï¼šåœ¨ #scraper-requests é¢‘é“å‘é€ URL å³å¯æŠ“å–"
        )
    
    async def on_shutdown(self):
        """Agent å…³é—­æ—¶æ‰§è¡Œ"""
        logger.info("ğŸŒ ç½‘é¡µæŠ“å–å™¨ å·²åœæ­¢")
    
    @on_event("workspace.messaging.channel_message_created")
    async def handle_channel_message(self, event):
        """å¤„ç†é¢‘é“æ¶ˆæ¯äº‹ä»¶"""
        try:
            payload = event.get("payload", {})
            channel = payload.get("channel")
            message_text = payload.get("text", "")
            thread_id = payload.get("thread_id")
            
            # åªå¤„ç† scraper-requests é¢‘é“çš„æ¶ˆæ¯
            if channel != "çµæ„Ÿé‡‡é›†":
                return
            
            # å¿½ç•¥è‡ªå·±å‘é€çš„æ¶ˆæ¯
            if payload.get("agent_id") == self.agent_id:
                return
            
            logger.info(f"Received scrape request: {message_text[:100]}")
            
            # æå– URL
            urls = self._extract_urls(message_text)
            
            if not urls:
                await self._send_channel_message(
                    "çµæ„Ÿé‡‡é›†",
                    "âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ URLï¼Œè¯·å‘é€ä¸€ä¸ªç½‘é¡µé“¾æ¥",
                    thread_id=thread_id
                )
                return
            
            # å¤„ç†ç¬¬ä¸€ä¸ª URL
            url = urls[0]
            
            # éªŒè¯ URL
            if not self.scraper.validate_url(url):
                await self._send_channel_message(
                    "çµæ„Ÿé‡‡é›†",
                    f"âŒ URL æ ¼å¼æ— æ•ˆï¼š{url}",
                    thread_id=thread_id
                )
                return
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if self.db.check_url_exists(url):
                await self._send_channel_message(
                    "çµæ„Ÿé‡‡é›†",
                    f"â„¹ï¸ è¯¥å†…å®¹å·²å­˜åœ¨äºçŸ¥è¯†åº“ä¸­\nğŸ”— {url}",
                    thread_id=thread_id
                )
                return
            
            # å‘é€å¤„ç†ä¸­æ¶ˆæ¯
            await self._send_channel_message(
                "çµæ„Ÿé‡‡é›†",
                f"ğŸ” æ­£åœ¨æŠ“å–...\nğŸ”— {url}",
                thread_id=thread_id
            )
            
            # æŠ“å–å†…å®¹
            result = await self._scrape_url(url)
            
            if result:
                await self._send_channel_message(
                    "çµæ„Ÿé‡‡é›†",
                    f"âœ… æŠ“å–æˆåŠŸï¼\n\nğŸ“„ **{result['title']}**\nğŸ“Š {result['word_count']} å­—\n\n_å·²æ·»åŠ åˆ°å†…å®¹åº“ï¼Œç­‰å¾… AI å¤„ç†..._",
                    thread_id=thread_id
                )
            else:
                await self._send_channel_message(
                    "çµæ„Ÿé‡‡é›†",
                    f"âŒ æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ URL æ˜¯å¦å¯è®¿é—®\nğŸ”— {url}",
                    thread_id=thread_id
                )
        
        except Exception as e:
            logger.error(f"Error handling message: {str(e)}")
    
    async def _scrape_url(self, url: str) -> dict:
        """
        æŠ“å– URL å¹¶ä¿å­˜
        
        Returns:
            æŠ“å–ç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # æŠ“å–å†…å®¹ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œä»¥é¿å…é˜»å¡ï¼‰
            loop = asyncio.get_event_loop()
            scraped_data = await loop.run_in_executor(
                None,
                self.scraper.scrape_url,
                url
            )
            
            if not scraped_data:
                return None
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            content_data = {
                'title': scraped_data['title'],
                'url': scraped_data['url'],
                'raw_content': scraped_data['content'],
                'source': scraped_data.get('source', 'Web'),
                'source_type': 'web'
            }
            
            content_id = self.db.add_content(content_data)
            
            if content_id:
                # å‘é€äº‹ä»¶
                await self._emit_content_discovered(content_id, content_data)
                
                # å‘é€åˆ° content-feed é¢‘é“
                await self._notify_new_content(content_data)
                
                # ç»Ÿè®¡å­—æ•°
                from tools.content_tools import ContentProcessor
                word_count = ContentProcessor.count_words(scraped_data['content'])
                
                return {
                    'title': scraped_data['title'],
                    'word_count': word_count,
                    'content_id': content_id
                }
            
            return None
            
        except Exception as e:
            logger.error(f"Error scraping URL {url}: {str(e)}")
            return None
    
    def _extract_urls(self, text: str) -> list:
        """ä»æ–‡æœ¬ä¸­æå– URL"""
        # åŒ¹é… http:// æˆ– https:// å¼€å¤´çš„ URL
        pattern = r'https?://[^\s<>"{}|\\^`\[\]]+'
        urls = re.findall(pattern, text)
        return urls
    
    async def _emit_content_discovered(self, content_id: str, content_data: dict):
        """å‘é€ content.discovered äº‹ä»¶"""
        # äº‹ä»¶ç³»ç»Ÿæš‚æ—¶ç¦ç”¨ï¼Œä½¿ç”¨é¢‘é“æ¶ˆæ¯é€šçŸ¥
        logger.debug(f"Content discovered: {content_id}")
    
    async def _notify_new_content(self, content_data: dict):
        """å‘é€æ–°å†…å®¹é€šçŸ¥åˆ°é¢‘é“"""
        title = content_data['title']
        url = content_data.get('url', '')
        source = content_data.get('source', 'Web')
        content = content_data.get('raw_content', '')
        
        # æˆªå–å†…å®¹é¢„è§ˆ
        preview = content[:200] + "..." if len(content) > 200 else content
        
        message = f"ğŸŒ **{title}**\n\n"
        message += f"ğŸ“ {preview}\n\n"
        message += f"ğŸ“š æ¥æºï¼š{source}\n"
        if url:
            message += f"ğŸ”— {url}\n"
        message += f"\n_ç­‰å¾… AI å¤„ç†ä¸­..._"
        
        await self._send_channel_message("çµæ„Ÿæ•æ‰‹", message)
    
    async def _send_channel_message(self, channel: str, text: str, thread_id: str = None):
        """å‘é€é¢‘é“æ¶ˆæ¯"""
        try:
            messaging = self.client.mod_adapters.get("openagents.mods.workspace.messaging")
            if messaging:
                await messaging.send_channel_message(
                    channel=channel,
                    text=text,
                    thread_id=thread_id
                )
        except Exception as e:
            logger.error(f"Failed to send channel message: {str(e)}")


async def main():
    """è¿è¡Œ Web Scraper Agent"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Web Scraper Agent")
    parser.add_argument("--host", default="localhost", help="Network host")
    parser.add_argument("--port", type=int, default=8700, help="Network port")
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿— - åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œç»ˆç«¯
    log_file = Path(__file__).parent.parent / 'logs' / 'agents' / 'web_scraper.log'
    log_file.parent.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='a'),
            logging.StreamHandler(sys.stdout)
        ],
        force=True
    )
    
    # å¼ºåˆ¶åˆ·æ–°è¾“å‡º
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)
    
    agent = WebScraperAgent()
    
    try:
        await agent.async_start(
            network_host=args.host,
            network_port=args.port,
        )
        
        logger.info(f"Web Scraper Agent running. Press Ctrl+C to stop.")
        
        # ä¿æŒè¿è¡Œ
        while True:
            await asyncio.sleep(1)
    
    except KeyboardInterrupt:
        logger.info("\nShutting down...")
    finally:
        await agent.async_stop()


if __name__ == "__main__":
    asyncio.run(main())